Date : 6/2/25

-   Airflow is the pipeline where you can process data from multiple data sources like RDBMS, NoSQL
    it is mlops ci/cd pipeline

-   Learning models do not allow multiple data sources to be accessed at one time so we require this platforms

-   Apache Airflow is used for the scheduling and orchestration of data pipelines or workflows. Orchestration of data pipelines referes to the
    sequencing, coordination, scheduling and managing of comples data pipelines from diverse sources.

-   pip install cookiecutter

-   creating a cookiecutter aiml template
    (venv) PS C:\Users\Ashutosh\OneDrive\Desktop\mlops-vac> cookiecutter https://github.com/drivendataorg/cookiecutter-data-science -c v1
  [1/8] project_name (project_name): aiml
  [2/8] repo_name (aiml): aiml
  [3/8] author_name (Your name (or your organization/company/team)): aissms
  [4/8] description (A short description of the project.): This is related to mlops
  [5/8] Select open_source_license
    1 - MIT
    2 - BSD-3-Clause
    3 - No license file
    Choose from [1/2/3] (1): 1
  [6/8] s3_bucket ([OPTIONAL] your-bucket-for-syncing-data (do not include 's3://')): 
  [7/8] aws_profile (default):
  [8/8] Select python_interpreter
    1 - python3
    2 - python
    Choose from [1/2] (1): 1


=============================================================================
*** DEPRECATION WARNING ***

Cookiecutter data science is moving to v2 soon, which will entail using
the command `ccds ...` rather than `cookiecutter ...`. The cookiecutter command
will continue to work, and this version of the template will still be available.
To use the legacy template, you will need to explicitly use `-c v1` to select it.

Please update any scripts/automation you have to append the `-c v1` option,
which is available now.

For example:
    cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science
=============================================================================

-   Copy the aiml folder to the main folder

-   Data collection -> Data pre-processing -> Data Splitting -> Model Selection

*******************************************************************************************************************************************

Date : 7/2/25

- Orchestration : The process of automatically organizing and combining data from different storage locations to make it ready for analysis

- Splitting of data : train and test,
  train : feature train data, target train data
  test : feature test data, target test data

- mlflow congif cmd : mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 127.0.0.1 -p 5000
                ui  : mlflow ui

- mlflow : An open platform to simplify the machine learning lifecycle

- MLflow components : Tracking, Projects, Models, Registry
  Tracking : record and query experiments
  Projects : package data science code in a format to reproduce runs on any platform
  Models : deploy ml models in diversing serving environments
  Registry : store, annotate, discover, manage models in a central repo.

- Django : A framework based on MVT(Model View template) design pattern

- csrf_token : Gives security whenever the page is loaded

//Commands to start and execute a django app and database connection

- django-admin startproject firstApp          ## Start a project   
- django-admin startapp firstApp              ## Start a app
- python manage.py sqlmigrate firstApp 0001   ## Connect through the migrations made to the data base
- python manage.py makemigrations             ## Creates migrations 
- python manage.py migrate                    ## An authentication command to connect admin and database to the project
- python manage.py runserver                  ## Run the django server


**********************************************************************************************************************************************

Date : 8/2/25

- Docker : code-buid-ship-run

- Docker : It is a container management system/service
  To easily develop applications, ship them into containers which can then be deployed anywhere

- Docker client : build, pull, run
- Docker Host : Docker daemon -> containers, images
- Docker Registry

- Docker image : A combination of a file system and parameters
  docker run hello-world                  ##command to print hello-world
  sudo docker run -it centos /bin/bash    ##Command for centos
  docker images                           ##See list of docker images on the system

- Docker networking : Allows to create a network of docker containers managed by a master node called the manager.
  sudo docker network
  sudo docker network ls

- Types of networking
  1. Bridge
  2. None
  3. Host

- Docker security best practices:
  Keep the host system and Docker software update
  Use official, Verified docker image from trusted sources
  Run containers as Non-Root Users - docker run -user 1000 mycontainer
  Limit the number of open ports on a container - docker run -p 80:80 myimage
  Use encryted environment variables and docker secrets
  Monitor the host and container logs : docker logs<container_name_or_id>
  Use network segementation and firewalls:
  Enable security features

- Commands:
  docker login
  docker build -t mlops-vac:1.0 .
  docker run -p 8000:8000 mlops-vac:1.0
  docker-compose up --build
